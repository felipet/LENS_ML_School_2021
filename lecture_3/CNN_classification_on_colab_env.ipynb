{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsPoDNTc_Cx9",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Steps:\" data-toc-modified-id=\"Steps:-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Steps:</a></span></li><li><span><a href=\"#General\" data-toc-modified-id=\"General-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>General</a></span></li></ul></li><li><span><a href=\"#DATA\" data-toc-modified-id=\"DATA-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>DATA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-and-take-a-look-at-the-data\" data-toc-modified-id=\"Download-and-take-a-look-at-the-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Download and take a look at the data</a></span></li><li><span><a href=\"#Data-directories\" data-toc-modified-id=\"Data-directories-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data directories</a></span></li></ul></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-the-model-layers\" data-toc-modified-id=\"Define-the-model-layers-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Define the model layers</a></span></li><li><span><a href=\"#Compile-the-model\" data-toc-modified-id=\"Compile-the-model-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Compile the model</a></span></li><li><span><a href=\"#Model-summary-and-plot\" data-toc-modified-id=\"Model-summary-and-plot-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Model summary and <em>plot</em></a></span></li><li><span><a href=\"#Train-the-model\" data-toc-modified-id=\"Train-the-model-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Train the model</a></span></li><li><span><a href=\"#Plot--the-learning-curves\" data-toc-modified-id=\"Plot--the-learning-curves-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Plot  the learning curves</a></span></li><li><span><a href=\"#Test-model-on-'test'-dataset\" data-toc-modified-id=\"Test-model-on-'test'-dataset-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Test model on <em>'test'</em> dataset</a></span></li></ul></li><li><span><a href=\"#Model-Improvement\" data-toc-modified-id=\"Model-Improvement-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model Improvement</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dropout\" data-toc-modified-id=\"Dropout-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Dropout</a></span></li><li><span><a href=\"#Augmentation\" data-toc-modified-id=\"Augmentation-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Augmentation</a></span></li></ul></li><li><span><a href=\"#Transfer-learning\" data-toc-modified-id=\"Transfer-learning-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Transfer learning</a></span></li><li><span><a href=\"#Extras\" data-toc-modified-id=\"Extras-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Extras</a></span><ul class=\"toc-item\"><li><span><a href=\"#Predict-on-single-images\" data-toc-modified-id=\"Predict-on-single-images-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Predict on single images</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnKORuD7_CyC"
   },
   "source": [
    "# Classification with Convolutional Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jV4c4wV3_CyD"
   },
   "source": [
    "The aim of this notebook is to showcase in practice - i.e. in code - how to build a simple 2D CNN for the purpose of classifying image data.\n",
    "\n",
    "The dataset used is intentionally a relatively small dataset, for the sake of time economy, but also in order to reduce the computational resources required.\n",
    "\n",
    "Remember: https://keras.io is a useful source of information regarding Keras and Tensorflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNBbIZ-6_CyD"
   },
   "source": [
    "###  Steps:\n",
    "\n",
    "1) DATA. As a first step we will load the original data and divide the available images into three datasets (training, validation, test). \n",
    "\n",
    "2) MODEL. Build the network architecture. This is the part where we put together the code which: describes the neural network, processes the images, and feeds the images into the network for training. The training of the model also happens in this step.\n",
    "\n",
    "3) LEARNING CURVES. Once the training is finished, plot the performance of the trained model (using 'accuracy' as our metric) in order to evaluate how well it did (or not).\n",
    "\n",
    "4) DROPOUT. A first effort to improve the performance of the model (if we observed overfitting earlier).\n",
    "\n",
    "5) AUGMENTATION. Another way to improve model accuracy or avoid overfitting\n",
    "\n",
    "6) TRANSFER LEARNING. Load and use an existing pretrained model for classifying the same dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2CF__FE_CyD"
   },
   "source": [
    "### General\n",
    "\n",
    "Let's start by importing **various useful** libraries in one go. \n",
    "We also check the versions of keras and tensorflow he have in our disposal (because we can).\n",
    "If more libraries become needed later on, come back to the cell below and add them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 963,
     "status": "ok",
     "timestamp": 1613216084596,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "Zg8WlzH4_CyE",
    "outputId": "823a8dff-10b0-44ff-cd9d-a7c7d014ae7e"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os, shutil\n",
    "from shutil import copyfile\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "print(keras.__version__)\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 986,
     "status": "ok",
     "timestamp": 1613216087604,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "ILjFG8p7h__3",
    "outputId": "d9e0d4d0-158c-4682-cd09-8b0772448844"
   },
   "outputs": [],
   "source": [
    "# if curious about the hardware resources google has reserved for you in this GPU session, execute this:\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNmGWwer_CyG"
   },
   "source": [
    "## DATA\n",
    "\n",
    "\n",
    "We will use an existing dataset, dowloaded from kaggle (https://www.kaggle.com/prasunroy/natural-images)\n",
    "\n",
    "This is a set of images of 8 different objects. \n",
    "The original dataset is modified a bit in order to have equal number of images in each class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzdfW14JLSl0"
   },
   "source": [
    "### Download and take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5458,
     "status": "ok",
     "timestamp": 1613215794870,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "JPlmJmLQvg7G",
    "outputId": "f620eff5-9b80-477f-ed00-69dda6cfb34b"
   },
   "outputs": [],
   "source": [
    "# download a zip file containing the image data we will use \n",
    "!gdown --id 13uS1xSm1p6mjiTU72paSAB70p3XKn3NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Id41Gd0DANT"
   },
   "outputs": [],
   "source": [
    "# unzip the file\n",
    "!unzip '/content/natural_images.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "executionInfo": {
     "elapsed": 1010,
     "status": "ok",
     "timestamp": 1613215871771,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "8o8NaLAr6vlr",
    "outputId": "a038f57b-53ed-4456-b873-04f0748d068f"
   },
   "outputs": [],
   "source": [
    "# let's randomly plot some of the images to see what they look like \n",
    "# (every time you execute this cell you will get a random image from your dataset)\n",
    "data_path='/content/natural_images'\n",
    "obj=random.choice(os.listdir(data_path))\n",
    "file=random.choice(os.listdir(os.path.join(data_path,obj)))\n",
    "\n",
    "print('This is an image of a(n):',obj)\n",
    "img=mpimg.imread(os.path.join(data_path,obj,file))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpEhSVfC_CyH"
   },
   "source": [
    "### Data directories\n",
    "\n",
    "**Create a hierarchy of directories**, with seperate directories for *training*,  *validation*, and *test* datasets. Each one of these will contain a subdirectory for each *class*.\n",
    "\n",
    "_Important_: __obviously, building the datasets only needs to be done once.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ozpJIF4_CyH"
   },
   "outputs": [],
   "source": [
    "# In the line below define as 'base' the entry dir of our session\n",
    "base='/content'\n",
    "\n",
    "# define the directory to store the datasets we will create soon\n",
    "classification_data_path=os.path.join(base + '/natural_images_classification') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "iQIgLSVG_CyH"
   },
   "outputs": [],
   "source": [
    "\n",
    "# create the directory tree described earlier\n",
    "if not os.path.exists(classification_data_path):\n",
    "  os.mkdir(classification_data_path)\n",
    "\n",
    "# define datasets (i.e. subdirs within the 'base' dir) and define classes (i.e. sub-subdirs for each of the dataset directories)\n",
    "datasets=['train','validation','test']\n",
    "classes=['airplane','car','cat','dog','flower','fruit','motorbike','person']\n",
    "\n",
    "# create that hierarchy of dirs and subdirs\n",
    "for dtype in datasets:\n",
    "  path_set=os.path.join(classification_data_path,dtype)\n",
    "  if not os.path.exists(path_set):\n",
    "    os.mkdir(path_set)\n",
    "  for c in classes:\n",
    "    path_class=os.path.join(path_set,c)\n",
    "    if not os.path.exists(path_class):\n",
    "      os.mkdir(path_class)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IijBVCqh_CyI"
   },
   "outputs": [],
   "source": [
    "# Function  - named 'pop_dirs' - for splitting the data into train, validation, and test data (after shuffling them) \n",
    "# and populating the directory tree created above.\n",
    "\n",
    "def pop_dirs(SOURCE, TRAINING, VALIDATION, TEST, SPLIT_SIZE):\n",
    "    # create list of all filenames in the SOURCE directory\n",
    "    files = []\n",
    "    for filename in os.listdir(SOURCE):\n",
    "        file = SOURCE + filename\n",
    "        if os.path.getsize(file) > 0:\n",
    "            files.append(filename)\n",
    "        else:\n",
    "            print(filename + \" is zero length, will skip.\") # if a file is empty, ignore it\n",
    "\n",
    "            \n",
    "    # First decide how many files should go in each dataset (based on the SPLIT_SIZE parameter).\n",
    "    # Then randomly assign which  filenames should go in each one of the datasets (shuffle namelinst before splitting them)        \n",
    "    training_length = int(len(files) * SPLIT_SIZE)               \n",
    "    validation_length = int((len(files) - training_length)/2.0)\n",
    "    test_length = validation_length\n",
    "    shuffled_set = random.sample(files, len(files))\n",
    "    training_set = shuffled_set[0:training_length]\n",
    "    validation_set = shuffled_set[training_length:training_length+validation_length]\n",
    "    test_set = shuffled_set[training_length+validation_length:training_length+validation_length+test_length]\n",
    "\n",
    "\n",
    "    # Based on the name lists created above, copy files into relevant dirs.\n",
    "    for filename in training_set:\n",
    "        this_file = SOURCE + filename\n",
    "        destination = TRAINING + filename\n",
    "        copyfile(this_file, destination)\n",
    "\n",
    "    for filename in validation_set:\n",
    "        this_file = SOURCE + filename\n",
    "        destination = VALIDATION + filename\n",
    "        copyfile(this_file, destination)\n",
    "\n",
    "    for filename in test_set:\n",
    "        this_file = SOURCE + filename\n",
    "        destination = TEST + filename\n",
    "        copyfile(this_file, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 934,
     "status": "ok",
     "timestamp": 1613218789536,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "PEj5M7gL_CyI",
    "outputId": "c550039f-f003-43bb-e108-5412913a0d62"
   },
   "outputs": [],
   "source": [
    "# The actual copying of files from the 'data' folder into the folder hierarchy created earlier. \n",
    "\n",
    "j=0\n",
    "# fraction of data (from 0.0 to 1.0) to be used as training data. The rest will be split equally a\n",
    "split_size = .7 \n",
    "\n",
    "\n",
    "for c in classes:\n",
    "\n",
    "  src_path=os.path.join(base,'natural_images',c)+'/'\n",
    "\n",
    "  train_path=os.path.join(classification_data_path,'train',c)+'/' \n",
    "  validation_path=os.path.join(classification_data_path,'validation',c)+'/'\n",
    "  test_path=os.path.join(classification_data_path,'test',c)+'/'\n",
    "\n",
    "  print('Copyting data FROM:')\n",
    "  print(src_path)\n",
    "  print('TO:')\n",
    "  print(train_path)\n",
    "  print(validation_path)\n",
    "  print(test_path)\n",
    "  print('\\n')  \n",
    "    \n",
    "  pop_dirs(src_path, train_path, validation_path, test_path, split_size)\n",
    "  j+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnFmAQUS_CyJ"
   },
   "source": [
    "Check the number of images for each dataset and each class.\n",
    "\n",
    "There should 700 images for each class.\n",
    "For a 70%-15%-15% split, you should have 490 training, 105 validation, and 105 test images for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1613218793265,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "BBaJdOeM_CyJ",
    "outputId": "b0e6210f-26dc-43fd-e3d1-b2fe4837214f"
   },
   "outputs": [],
   "source": [
    "datasets=['train','validation','test']\n",
    "classes=['airplane','car','cat','dog','flower','fruit','motorbike','person']\n",
    "\n",
    "for dset in datasets:\n",
    "  for c in classes:\n",
    "    path=os.path.join(base,'natural_images_classification',dset,c)\n",
    "    print('number of',  dset,  'images in', c,':', len(os.listdir(path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nh1w8Z5djOZe"
   },
   "source": [
    "**EXCELLENT!** You now have generated three dataset directories, with the data needed for training and evaluating your model. **Let's now build the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdKBxd_Y_CyJ"
   },
   "source": [
    "## Model\n",
    "\n",
    "This section containes the code for the CNN model architecture, loading the data into the model, compiling the model, and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Btd011ZI_CyJ"
   },
   "outputs": [],
   "source": [
    "# Run ONLY when clearing of the session is needed\n",
    "\n",
    "tensorflow.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xg7_3Sf1_CyK"
   },
   "outputs": [],
   "source": [
    "# Place needed and updatable parameters in a dictionary for easy access and change\n",
    "# Changing model hyperparameters such as learning rate, number of epochs, batch size, input size etc. can help improve the performance of a model\n",
    "# You can experiment with these values by changing their entries below.\n",
    "PARAMS = {'lr': 1e-3, # Learning Rate\n",
    "          'batch_size': 64,\n",
    "          'n_epochs': 35,  # 30 to 50 epochs is a good range for our example\n",
    "          'loss': 'categorical_crossentropy',\n",
    "          'metrics': 'acc', # accuracy\n",
    "          'image_input_shape' : (100,100,3),\n",
    "          'save_model' : 'classification_1.h5' \n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 705,
     "status": "ok",
     "timestamp": 1613218811230,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "kbl_b8Tq_CyK",
    "outputId": "2427a6bc-0f60-4010-e25e-0f31a2ff2c51"
   },
   "outputs": [],
   "source": [
    "# Use the ImageDataGenerator class from Keras, and its flow_from_directory method to generate shuffled data batches\n",
    "# Notice that the images are rescaled (division with 255) so that each pixel value ranges betwee 0 and 1.\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1.0/255.)\n",
    "validation_datagen = ImageDataGenerator(rescale = 1.0/255.)\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0/255.)\n",
    "\n",
    "train = train_datagen.flow_from_directory(\n",
    "        classification_data_path+'/train',\n",
    "        target_size=(100, 100),\n",
    "        batch_size=PARAMS['batch_size'],\n",
    "        class_mode='categorical') \n",
    "\n",
    "validation = validation_datagen.flow_from_directory(\n",
    "        classification_data_path+'/validation',\n",
    "        target_size=(100, 100),\n",
    "        batch_size=PARAMS['batch_size'],\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "test = test_datagen.flow_from_directory(\n",
    "        classification_data_path+'/test',\n",
    "        target_size=(100, 100),\n",
    "        batch_size=PARAMS['batch_size'],\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1613218813439,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "j-B6z58t_CyK",
    "outputId": "cf49bedf-3f4a-408e-86b9-f1d331cd2c5f"
   },
   "outputs": [],
   "source": [
    "# Check the sizes of the image and label batches (useful for debugging size errors)\n",
    "for images_batch, labels_batch in train:\n",
    "    print('Image batches have shape:', images_batch.shape)\n",
    "    print('Label batches have shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPBQrcLJ_CyL"
   },
   "source": [
    "### Define the model layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lwBHftR_CyL"
   },
   "source": [
    "Use Conv2D layers, each of wich will be followed by MaxPooling2D layer. \n",
    "\n",
    "- How many Conv2D layers to use? (in this case **3 is a good guess** to start with)\n",
    "- Number of filters for each? (powers of two typically used, with number of filters typically increassing in deeper layers, i.e. 32,64,128)\n",
    "- Size of these filters? 3x3, or 5x5, or 7x7 are typical values (start with **3x3**)\n",
    "- A Flatten layer should be added after the last Conv2D + MaxPooling layer \n",
    "- Last layer should be a Dense layer with a 'softmax' activation, as we are building a 'classification' model. The number of nodes for this layer should of course be equal to the number of classes in our data.\n",
    "- It is likely that more Dense layers can be added between the Flatten and the last Dense layer (the classifier).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTgwTjOZ_CyL"
   },
   "source": [
    "<font color='blue'> **Task 1:**\n",
    "+ Build the CNN, by adding the necessary sequence of layers: use what was discussed in the lecture and in the text just above as quide. \n",
    "You should build something like this: </font>\n",
    "\n",
    "<font color='blue'>\n",
    "Conv2D + MaxPooling --> Conv2D + MaxPooling --> Conv2D + MaxPooling --> Flatten --> Dense --> Dense(final classifier with softmax activation)\n",
    "</font>\n",
    "\n",
    "**The first convolutional layer (input layer) and following maxpooling is already in place as a guide. NOTE: only input layer needs the 'Input_shape argument'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVPDREC__CyL"
   },
   "outputs": [],
   "source": [
    "# The model layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=PARAMS['image_input_shape']))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "### add code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ALsUkWZ_CyL"
   },
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z120r3Vy_CyM"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=PARAMS['loss'],\n",
    "              optimizer=optimizers.Adam(lr=PARAMS['lr']),\n",
    "              metrics=[PARAMS['metrics']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JURbIg76_CyM"
   },
   "source": [
    "### Model summary and *plot*\n",
    "\n",
    "Draw a summary of each layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 477,
     "status": "ok",
     "timestamp": 1613218820202,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "jlivBHF1_CyM",
    "outputId": "03140252-5209-4d2a-866e-217b1fe7b491"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKs5uQks_CyM"
   },
   "source": [
    "Plot a graph of the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1114,
     "status": "ok",
     "timestamp": 1613218822619,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "qG9l0owZ_CyM",
    "outputId": "f28c051c-3bf3-4271-a204-b484059f3623"
   },
   "outputs": [],
   "source": [
    "# create a graph of the model layers and save a png image of it\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png',show_shapes=True,rankdir='TB',show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTCDvHJW_CyN"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 259708,
     "status": "ok",
     "timestamp": 1613219084508,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "43K6bOWP_CyN",
    "outputId": "b728985f-336d-4a70-b5d6-ae6c58490c07"
   },
   "outputs": [],
   "source": [
    "# Fit (i.e train) the model \n",
    "\n",
    "spe=len(train)   # Steps Per Epoch: train_size/batch_size\n",
    "epoch_num=PARAMS['n_epochs']\n",
    "val_steps=len(validation)  # validation_size/batch_size\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "            train,\n",
    "            steps_per_epoch=spe,  \n",
    "            epochs=epoch_num,\n",
    "            validation_data=validation,\n",
    "            validation_steps=val_steps) \n",
    "\n",
    "\n",
    "# To save the output of the model, uncomment the line below.\n",
    "#model.save(PARAMS['save_model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_feMUeX_CyN"
   },
   "source": [
    "### Plot  the learning curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "executionInfo": {
     "elapsed": 882,
     "status": "ok",
     "timestamp": 1613219093605,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "cCA6ZEpl_CyN",
    "outputId": "7d18e362-ed71-4b31-a088-aca180536590"
   },
   "outputs": [],
   "source": [
    "# Check the progress of the training, by plotting the Learning Curves:\n",
    "# a) Accuracy of the model on the train and validation datasets as a function of epochs (time)\n",
    "# b) Value of loss function on train and validation datasets as a function of epochs\n",
    "\n",
    "# if the model was saved from an earlier run, uncomment the line below to load it\n",
    "#model = load_model('name_of_model_to_load.h5')\n",
    "\n",
    "epoch_num=PARAMS['n_epochs']\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(range(epoch_num), acc, 'b', label='Training accuracy')\n",
    "plt.plot(range(epoch_num), val_acc, 'g', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(epoch_num), loss, 'b', label='Training loss')\n",
    "plt.plot(range(epoch_num), val_loss, 'g', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65WnhGIA_CyN"
   },
   "source": [
    "### Test model on *'test'* dataset\n",
    "\n",
    "As a final step on checking the performance of the model, we run the trained model on the third dataset, the 'test' dataset. (These are data that our model has not seen yet).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1657,
     "status": "ok",
     "timestamp": 1613219104187,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "Q039NuUp_CyO",
    "outputId": "f27534ed-5657-4105-8591-a42839e288ff"
   },
   "outputs": [],
   "source": [
    "test_score = model.evaluate(test)\n",
    "print(model.metrics_names)\n",
    "print(test_score)\n",
    "print()\n",
    "print(\"Accuracy = \",test_score[1]*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KptxoEqwMku_"
   },
   "source": [
    "## Model Improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "019kJObq_CyO"
   },
   "source": [
    "### Dropout\n",
    "\n",
    "\n",
    "One way to overcome overfitting of the model is to add one or more Dropout layers in the model. (https://keras.io/api/layers/regularization_layers/dropout/)\n",
    "\n",
    "A 'Dropout' layer will randomly drop-out (i.e. remove) a user-defined number of nodes (i.e. connections) between two layers.\n",
    "\n",
    "Adding a dropout layer looks like this:\n",
    "    *model.add(layers.Dropout(0.5))*\n",
    "The argument of Dropout is a number from 0.0 to 1.0 which corresponds to the fraction of nodes to **remove** . \n",
    "\n",
    "A typical value  is around 0.5-0.6 (i.e. 50-60% or the nodes removed).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PJwLE0k_CyO"
   },
   "source": [
    "<font color='blue'>**Task 2**\n",
    "Add a Dropout Layer in the model you created earlier and train the model again (add the Dropout layer after a Dense layer).\n",
    "How do the learning curves compare to those of the previous model?\n",
    "\n",
    "**NOTE:** \n",
    " + remember to clear the session with the command available at the beginning of Section 2\n",
    " + must run again the cell with the model architecture, and compile the model again before fitting the new architecture. \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hl2CGtfs_CyO"
   },
   "outputs": [],
   "source": [
    "# clear the session\n",
    "\n",
    "### add code here:\n",
    "tensorflow.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nb177jjk_CyO"
   },
   "outputs": [],
   "source": [
    "# model layers\n",
    "# use the previous model PLUS a (or two) dropout layer\n",
    "\n",
    "\n",
    "### add code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZhv5APj_CyO"
   },
   "outputs": [],
   "source": [
    "# compile the model\n",
    "\n",
    "### add code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 256674,
     "status": "ok",
     "timestamp": 1613219408514,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "lidb-0Bl_CyO",
    "outputId": "9fedc455-3b7b-4bdb-afbb-c0bc5b23b370"
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "spe=len(train)   # Steps Per Epoch: train_size/batch_size\n",
    "epoch_num=PARAMS['n_epochs']\n",
    "val_steps=len(validation)  # validation_size/batch_size\n",
    "\n",
    "### add code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "executionInfo": {
     "elapsed": 829,
     "status": "ok",
     "timestamp": 1613219420171,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "w6fWr72g_CyO",
    "outputId": "31b51085-3a95-44be-b90c-ad7c7995243b"
   },
   "outputs": [],
   "source": [
    "# plot the learning curves\n",
    "\n",
    "### add code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1735,
     "status": "ok",
     "timestamp": 1613219426412,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "3L0NFip2_CyP",
    "outputId": "ded17ed0-dd07-4ba8-d533-ea7c8066a1dd"
   },
   "outputs": [],
   "source": [
    "# evaluate model on *test* dataset\n",
    "\n",
    "### add code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca2O54kz_CyP"
   },
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugwKqK0R_CyP"
   },
   "source": [
    "One possible remedy for overfitting, when it is caused by lack of enough data, is to use data augmentation.\n",
    "In data augmentation, already existing data - images in our case - are 'tranformed' into new data through some basic transformations like mirroring or flipping, rotation, zoom, rescaling, etc.  of the image.\n",
    "\n",
    "\n",
    "In Keras, the ImageDataGenerator class, offers an easy way to include these augmentation techniques.\n",
    "Check the ImageDataGenerator entry here:https://keras.io/api/preprocessing/image/  to see how it is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RabioHxh_CyP"
   },
   "source": [
    "<font color='blue'>**Task 3**\n",
    "-Based on the Keras link above, choose some augmentation transformations (zoom, flip, rotation) which make sense for the dataset you are using, and implement these transformations within the ImageDataGenerator.         \n",
    "**NOTE**: augmentation is applied only on the training dataset   \n",
    "-Train again the model and plot its performance on the new, augmented dataset.  \n",
    "-Do not forget to also evaluate the trained model on the 'test' dataset</font>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtafC4_c_CyP"
   },
   "outputs": [],
   "source": [
    "# clear the session\n",
    "\n",
    "tensorflow.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 532,
     "status": "ok",
     "timestamp": 1613219455494,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "9F7bnRja_CyP",
    "outputId": "c756006c-7533-4bc2-ca02-9dec3dc4b11a"
   },
   "outputs": [],
   "source": [
    "\n",
    "# add arguments that define augmentation techniques in the ImageDataGenerator line below\n",
    "\n",
    "### complete the code below:\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255. ### complete with the right arguments) \n",
    "\n",
    "# this can be left as is\n",
    "train = train_datagen.flow_from_directory(\n",
    "        classification_data_path+'/train',\n",
    "        target_size=(100, 100),\n",
    "        batch_size=PARAMS['batch_size'],\n",
    "        class_mode='categorical') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNcsR16L_CyQ"
   },
   "outputs": [],
   "source": [
    "# use the same model layers which you used the first time (i.e. no dropout layer)\n",
    "\n",
    "### add code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MeOuDIYi_CyQ"
   },
   "outputs": [],
   "source": [
    "# compile model \n",
    "model.compile(loss=PARAMS['loss'],\n",
    "              optimizer=optimizers.Adam(PARAMS['lr']),\n",
    "              metrics=[PARAMS['metrics']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 515990,
     "status": "ok",
     "timestamp": 1613219977830,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "u9f8p3GL_CyQ",
    "outputId": "b96dac8c-7c91-42ed-a5e2-a813f84e1222"
   },
   "outputs": [],
   "source": [
    "# Fit the model \n",
    "spe=len(train)   # train_size/batch_size\n",
    "epoch_num=PARAMS['n_epochs']\n",
    "val_steps=len(validation)  # validation_size/batch_size\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "            train,\n",
    "            steps_per_epoch=spe,  \n",
    "            epochs=epoch_num,\n",
    "            validation_data=validation,\n",
    "            validation_steps=val_steps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "executionInfo": {
     "elapsed": 915,
     "status": "ok",
     "timestamp": 1613220070038,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "Ikt8C5Ht_CyQ",
    "outputId": "d11acdfa-32f4-4a22-febd-2b3299790f1c"
   },
   "outputs": [],
   "source": [
    "# plot the learning curves\n",
    "\n",
    "\n",
    "epoch_num=PARAMS['n_epochs']\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(range(epoch_num), acc, 'b', label='Training accuracy')\n",
    "plt.plot(range(epoch_num), val_acc, 'g', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(epoch_num), loss, 'b', label='Training loss')\n",
    "plt.plot(range(epoch_num), val_loss, 'g', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1736,
     "status": "ok",
     "timestamp": 1613220077396,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "Fuqb5ga7_CyQ",
    "outputId": "27b83da4-d2ad-4765-8f31-dd8948d08425"
   },
   "outputs": [],
   "source": [
    "# evaluate model on *test* dataset\n",
    "\n",
    "test_score = model.evaluate(test)\n",
    "print(model.metrics_names)\n",
    "print(test_score)\n",
    "print()\n",
    "print(\"Accuracy = \",test_score[1]*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HlHEmul_CyR"
   },
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_G3Qona_CyR"
   },
   "source": [
    "We will use the VGG16 network (https://arxiv.org/abs/1409.1556) trained on the ImageNet dataset (http://image-net.org/about-overview).\n",
    "As it is already part of the Keras library, it is easy to load.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KodaXp9u_CyR"
   },
   "outputs": [],
   "source": [
    "# clear the prevoius session if needed\n",
    "tensorflow.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPnRZOlT_CyR"
   },
   "outputs": [],
   "source": [
    "# Import VGG16\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 947,
     "status": "ok",
     "timestamp": 1613220098148,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "bcRjJI6p_CyR",
    "outputId": "6aaf827f-7535-400f-a7c7-94f6659ee618"
   },
   "outputs": [],
   "source": [
    "# Create a copy of the VGG16 network, with input_size to match our dataset, and without including the last output layer(s)\n",
    "pretrained_net = VGG16(input_shape=(100,100,3),include_top=False,weights='imagenet',pooling='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 527,
     "status": "ok",
     "timestamp": 1613220101201,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "SA-uujM2_CyR",
    "outputId": "1b55292c-da44-42a3-e800-7f3bd9946c99",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the summary of the pretrained network, named 'pretrained_net'\n",
    "\n",
    "### add code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COWdmpZK_CyR"
   },
   "outputs": [],
   "source": [
    "# 'Freeze' (i.e. make them untrainable) all layers in thre pretrained network\n",
    "for layer in pretrained_net.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOr9BtCm_CyS"
   },
   "outputs": [],
   "source": [
    "# Add a couple of layers at the end of the pretrained network: we need to add our own output layer, \n",
    "# which will classify our data according to the number of classes we have.\n",
    "\n",
    "model2 = models.Sequential()\n",
    "model2.add(pretrained_net)\n",
    "model2.add(layers.Dense(8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1613220113499,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "6y_R-nRk_CyS",
    "outputId": "14ac3dc0-aad3-4de1-f3e3-761bf0b505ec"
   },
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EpPAWG5_CyS"
   },
   "source": [
    "<font color='blue'> **Task 4:**\n",
    "   \n",
    "- Train the pretrained network (in reallity only the layers we added will be trained, we 'froze' all the layers previous to those) with the same data as before and compaire its performance to that of the previous model.\n",
    "- NOTE: for the 'train' set, we used augmentation in the previous section. Do not use augmentation here.      \n",
    "* Add code for plotting the learning curves for this model (you can also choose to overplot the learning curves of the previous model for direct comparison)\n",
    "- Test the accuracty of the model on the 'test' dataset.</font> \n",
    "\n",
    "<font color='blue'>**Notice** that in this section (transfer learning) we named our model '**model2**', to distinquish it from the model we were building earlier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the augmentation arguments used earlier for the train dataset:\n",
    "\n",
    "### complete the code below:\n",
    "train_datagen =  \n",
    "\n",
    "train =  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fz5wQevv_CyS"
   },
   "outputs": [],
   "source": [
    "# compile model \n",
    "\n",
    "### add code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 560154,
     "status": "ok",
     "timestamp": 1613220716067,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "Qs0CN432_CyS",
    "outputId": "7cb78a28-acb1-4d6f-bd17-c5ea849bc39b"
   },
   "outputs": [],
   "source": [
    "# Fit the model \n",
    "\n",
    "\n",
    "### add code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "executionInfo": {
     "elapsed": 833,
     "status": "ok",
     "timestamp": 1613220792683,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "_zNu4bd9_CyS",
    "outputId": "28293c2d-c037-46e7-a844-28f69d22dc1f"
   },
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "epoch_num=PARAMS['n_epochs']\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(range(epoch_num), acc, 'b', label='Training accuracy')\n",
    "plt.plot(range(epoch_num), val_acc, 'g', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(epoch_num), loss, 'b', label='Training loss')\n",
    "plt.plot(range(epoch_num), val_loss, 'g', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1928,
     "status": "ok",
     "timestamp": 1613220797381,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "-wZ6U2bo_CyS",
    "outputId": "bb334d80-f263-49ec-e0ac-48921ecd7c63"
   },
   "outputs": [],
   "source": [
    "# evaluate trained model on 'test' dataset\n",
    "\n",
    "\n",
    "test_score = model2.evaluate(test)\n",
    "print(model2.metrics_names)\n",
    "print(test_score)\n",
    "print()\n",
    "print(\"Accuracy = \",test_score[1]*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F17qwZ5BNi3E"
   },
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58ZAzrsyORSv"
   },
   "source": [
    "### Predict on single images\n",
    "\n",
    "(requires trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 1089,
     "status": "ok",
     "timestamp": 1613221009359,
     "user": {
      "displayName": "Emmanouela Rantsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIXt3enj62I7JyiBeZnHHvn_FNnwR-VMYvrP3f_g=s64",
      "userId": "03285249317399815906"
     },
     "user_tz": -60
    },
    "id": "iFeEE0D60Z9m",
    "outputId": "48105aae-c4a2-4c83-c4bf-89a62eb7a622"
   },
   "outputs": [],
   "source": [
    "# After having trained a model, how to we call it and use it with individual inputs?\n",
    "# here's one way to do it:\n",
    "\n",
    "\n",
    "# load an image, transorm it into the right input shape for the model\n",
    "\n",
    "data_path='/content/natural_images'\n",
    "obj=random.choice(os.listdir(data_path))\n",
    "file=random.choice(os.listdir(os.path.join(data_path,obj)))\n",
    "\n",
    "img = image.load_img(os.path.join(data_path,obj,file), target_size=(100, 100))\n",
    "img_exp = image.img_to_array(img)\n",
    "img_exp = np.expand_dims(img_exp, axis=0)\n",
    "img_exp /= 255.\n",
    "\n",
    "## check the image\n",
    "plt.imshow(img_exp[0])\n",
    "plt.show()\n",
    "\n",
    "## ask model to predict\n",
    "## notice that the prediction comes in the form of a vector with length = number_of_classes and values which correspond to probablities for each class\n",
    "prediction = model.predict(img_exp)\n",
    "print(prediction[0])\n",
    "\n",
    "# just for fun: translate above prediction (vector with class probablilities) to a more human-friendly answer\n",
    "class_labels=train.class_indices\n",
    "class_labels=dict((value,key) for key,value in class_labels.items())\n",
    "predicted_class=class_labels[np.argmax(prediction[0],axis=-1)]\n",
    "accuracy=np.max(prediction)\n",
    "print('I am', accuracy*100,'%' ' certain that this is a ', predicted_class)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_classification_on_colab_env.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code is by no means original and is heavily based on https://www.tensorflow.org/tutorials/keras/classification. It serves as the \"Hello World\" programming example for a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################\n",
    "### Importing ###\n",
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to import all the tools which have made ML easy in the last few years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf #Tensorflow has made ML simple\n",
    "from tensorflow import keras #Keras makes using tensorflow even easier!\n",
    "import numpy as np #Everything needs numpy\n",
    "import matplotlib.pyplot as plt #For plotting\n",
    "print(tf.__version__) # Check we are running tensorflow version >2.0 (there were major changes between 1.x and 2.x)\n",
    "import seaborn as sns #makes plotting prettier, optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################\n",
    "### Data loading and normalisation ###\n",
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras comes with some datasets which have been curated for us. In this example we are using 70,000 greyscale images of different fashion items (an alternative to character recognition). There are 10 different catagories and the dataset even comes with its own data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images,train_labels), (test_images,test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our class names, these are for our benefit so we know what the numerical labels mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets inspect the data a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of training set = \", train_images.shape)\n",
    "print(\"First label = \", train_labels[0])\n",
    "print(\"Size of label set = \", train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has been split into 60,000 training images and 10,000 test images --> Note no validation here (yet)\n",
    "We can tell that each image is 28x28 pixels (not high resolution, but you will see why shortly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.gca().grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that, as expected, the first entry in the dataset is an \"Ankle boot\", but what actually is this image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect some pixels:\n",
    "print(\"Corner pixel = \",train_images[0][0,0])\n",
    "print(\"A middle pixel = \", train_images[0][10,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images/255.0\n",
    "test_images = test_images/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to normalise these pixel values to be [0,1] rather than [0,255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i],cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################\n",
    "### Building a network! ###\n",
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([keras.layers.Flatten(input_shape=(28,28)),\n",
    "                         keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "                          keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################\n",
    "### Training the network ###\n",
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"ADAM\",\n",
    "             loss = \"sparse_categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"]) #This is the one hint in this example that keras/tensorflow is not just straight python. \n",
    "# We must compile the model before we run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() #A nice summary to show us how big our network is, and how many parameters it has "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=10) #Now we set it training, you will see why we set this as a variable soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notice that the more we train the better our accuracy becomes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But our test accuracy is not as good "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################\n",
    "### Examine the training ###\n",
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can look at the history of the training\n",
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1) #Giving us something to plot against\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "plt.plot(epochs, test_acc*np.ones(len(epochs)), 'r', label='Final test accuracy')\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is the gap between training and test accuracy increasing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################\n",
    "### Examine the predictions ###\n",
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)\n",
    "print(predictions[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 25 test images, their predicted label, and the true label\n",
    "# Color correct predictions in blue, incorrect predictions in red. True labels are in brackets\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid('off')\n",
    "    plt.imshow(test_images[i], cmap=plt.cm.binary)\n",
    "    predicted_label = np.argmax(predictions[i])\n",
    "    true_label = test_labels[i]\n",
    "    if predicted_label == true_label:\n",
    "      color = 'blue'\n",
    "    else:\n",
    "      color = 'red'\n",
    "    plt.xlabel(\"{} ({})\".format(class_names[predicted_label], \n",
    "                                  class_names[true_label]),\n",
    "                                  color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################\n",
    "### Enable single predictions ###\n",
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test a single image we see that is it (as expected) a 28x28 pixel object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = test_images[0]\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, to use model.predict we need to make it a collection (of one), so we give it an extra dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.expand_dims(img,0)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can predict on this:\n",
    "predictions = model.predict(img)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not exactly what we want still, so we just say that the most likely class is the one we choose. For more complex problems we are able to set thresholds here instead of a straight choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictions[0]\n",
    "print(class_names[np.argmax(prediction)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################\n",
    "### Tasks ###\n",
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Play around with the network architecture and see if you can increase the accuracy for training and test: try a different optimizer, more/less epochs or more/less nodes or hidden layers (max 5 mins playing advised)\n",
    "\n",
    "2. Split the training data into a training and validation set, then implement a validation step during training (hint look at the keras API documentation for model.fit - this step is easier than you think)\n",
    "\n",
    "3. Plot the training and validation loss/accuracy with epoch\n",
    "\n",
    "4. Change the prediction to admit when it doesn't really know, say based on some threshold of confidence (examine one it got wrong for example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
